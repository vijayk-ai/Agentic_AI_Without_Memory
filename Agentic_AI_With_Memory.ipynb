{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSZlyekCAqKh"
   },
   "source": [
    "================================================================================\n",
    "AGENTIC AI AUTONOMOUS SYSTEM v2 ‚Äî WITH FULL MEMORY ARCHITECTURE\n",
    "================================================================================\n",
    "Framework:  LangChain + LangGraph\n",
    "Platform:   Google Colab\n",
    "Memory:     Short-Term (State) + Long-Term (FAISS) + Semantic (KB) +\n",
    "            Episodic (Run History) + Procedural (Learned Strategies)\n",
    "\n",
    "Agents:     Orchestrator ‚Üí Research ‚Üí Writer ‚Üí HTML Builder ‚Üí QA Reviewer\n",
    "            All agents read/write from the shared Memory Manager.\n",
    "\n",
    "HOW TO USE:\n",
    "1. Open this file in Google Colab (copy each cell block into separate cells)\n",
    "2. Add API keys in Colab Secrets (üîë sidebar):\n",
    "   - OPENAI_API_KEY  (or GOOGLE_API_KEY for Gemini)\n",
    "   - TAVILY_API_KEY  (free at https://tavily.com)\n",
    "3. Run all cells sequentially\n",
    "================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18269,
     "status": "ok",
     "timestamp": 1771080895034,
     "user": {
      "displayName": "Partha Bhowmick",
      "userId": "16205602225789129380"
     },
     "user_tz": 0
    },
    "id": "o88YmzGkA9uM",
    "outputId": "442ce682-fdd6-4dae-ff7f-d2d910804d30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/85.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.8/85.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/2.5 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m500.1/500.1 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Step 1: INSTALL DEPENDENCIES\n",
    "\n",
    "!pip install -q langchain langchain-openai langchain-community \\\n",
    "langchain-google-genai langgraph tavily-python faiss-cpu\n",
    "\n",
    "\n",
    "print(\"üöÄ Agentic AI With Memory Script Started\")\n",
    "\n",
    "def main():\n",
    "    print(\"Memory-enabled pipeline running...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 624,
     "status": "ok",
     "timestamp": 1771080897001,
     "user": {
      "displayName": "Partha Bhowmick",
      "userId": "16205602225789129380"
     },
     "user_tz": 0
    },
    "id": "MCBtUkJlBPwv",
    "outputId": "a1d29c48-1ef4-4609-c02f-dd8f4abcc93a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " API keys loaded from Colab Secrets\n",
      "üîß Config: openai / gpt-4o\n",
      "Memory dir: ./agent_memory\n"
     ]
    }
   ],
   "source": [
    "# Step 2: CONFIGURATION & API KEYS\n",
    "\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Load API keys\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
    "    os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"TAVILY_API_KEY\")\n",
    "    # os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
    "    print(\" API keys loaded from Colab Secrets\")\n",
    "except Exception:\n",
    "    print(\"  Set API keys manually or via Colab Secrets\")\n",
    "\n",
    "CONFIG = {\n",
    "    \"llm_provider\": \"openai\",          # \"openai\" or \"google\"\n",
    "    \"model_name\": \"gpt-4o\",            # or \"gemini-1.5-flash\"\n",
    "    \"temperature\": 0.3,\n",
    "    \"max_search_results\": 8,\n",
    "    \"max_revisions\": 2,\n",
    "    \"topic\": \"Latest AI News and Breakthroughs 2025-2026\",\n",
    "    # Memory paths (persisted to Colab filesystem)\n",
    "    \"memory_dir\": \"./agent_memory\",\n",
    "    \"vectorstore_path\": \"./agent_memory/vectorstore\",\n",
    "    \"knowledge_base_path\": \"./agent_memory/knowledge_base.json\",\n",
    "    \"episodes_path\": \"./agent_memory/episodes.json\",\n",
    "    \"strategies_path\": \"./agent_memory/strategies.json\",\n",
    "}\n",
    "\n",
    "# Create memory directory\n",
    "Path(CONFIG[\"memory_dir\"]).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"üîß Config: {CONFIG['llm_provider']} / {CONFIG['model_name']}\")\n",
    "print(f\"Memory dir: {CONFIG['memory_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16379,
     "status": "ok",
     "timestamp": 1771080920823,
     "user": {
      "displayName": "Partha Bhowmick",
      "userId": "16205602225789129380"
     },
     "user_tz": 0
    },
    "id": "qlwKbduYBk3A",
    "outputId": "dc7bd5c0-bb6f-45d7-f0c1-54f3bff9d00f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LLM, Embeddings, and Search tool initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-68099356.py:24: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  search_tool = TavilySearchResults(\n"
     ]
    }
   ],
   "source": [
    "# Step 3: IMPORTS & LLM SETUP\n",
    "from typing import TypedDict, Annotated, List, Dict, Optional\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "import operator\n",
    "\n",
    "# Initialize LLM\n",
    "if CONFIG[\"llm_provider\"] == \"openai\":\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    llm = ChatOpenAI(model=CONFIG[\"model_name\"], temperature=CONFIG[\"temperature\"])\n",
    "elif CONFIG[\"llm_provider\"] == \"google\":\n",
    "    from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "    llm = ChatGoogleGenerativeAI(model=CONFIG[\"model_name\"], temperature=CONFIG[\"temperature\"])\n",
    "\n",
    "# Initialize embeddings for long-term memory\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Initialize search tool\n",
    "search_tool = TavilySearchResults(\n",
    "    max_results=CONFIG[\"max_search_results\"],\n",
    "    search_depth=\"advanced\",\n",
    "    include_answer=True,\n",
    ")\n",
    "\n",
    "print(\" LLM, Embeddings, and Search tool initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1771080979384,
     "user": {
      "displayName": "Partha Bhowmick",
      "userId": "16205602225789129380"
     },
     "user_tz": 0
    },
    "id": "RVCqW3iGB0hw",
    "outputId": "2fee9908-97a1-49ca-8eb6-eb521a448cf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Vector Store: created new (empty)\n",
      "\n",
      " Memory system fully initialized!\n"
     ]
    }
   ],
   "source": [
    "class MemoryManager:\n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.vectorstore = None  # Initialized later\n",
    "        self.knowledge_base = self._load_json(config[\"knowledge_base_path\"], default={\"entities\": {}, \"source_reliability\": {}})\n",
    "        self.episodes = self._load_json(config[\"episodes_path\"], default=[])\n",
    "        self.strategies = self._load_json(config[\"strategies_path\"], default={\n",
    "            \"total_runs\": 0,\n",
    "            \"avg_quality_score\": 0,\n",
    "            \"best_query_patterns\": [],\n",
    "            \"preferred_html_template\": \"\",\n",
    "            \"writer_tips\": []\n",
    "        })\n",
    "        self._init_vectorstore()\n",
    "\n",
    "    def _load_json(self, path: str, default=None):\n",
    "        try:\n",
    "            with open(path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "                # Ensure the loaded data matches the default type if default is a list\n",
    "                if isinstance(default, list) and not isinstance(data, list):\n",
    "                    print(f\"    Warning: '{path}' contained an unexpected type ({type(data)}), returning default list.\")\n",
    "                    return default\n",
    "                return data\n",
    "        except (FileNotFoundError, json.JSONDecodeError):\n",
    "            return default if default is not None else {}\n",
    "\n",
    "    def _save_json(self, path: str, data):\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2, default=str)\n",
    "\n",
    "    # ‚îÄ‚îÄ 2. LONG-TERM MEMORY (Vector Store) ‚îÄ‚îÄ\n",
    "\n",
    "    def _init_vectorstore(self):\n",
    "        \"\"\"Load or create FAISS vector store.\"\"\"\n",
    "        try:\n",
    "            from langchain_community.vectorstores import FAISS\n",
    "            vs_path = self.config[\"vectorstore_path\"]\n",
    "            if Path(vs_path).exists() and Path(f\"{vs_path}/index.faiss\").exists():\n",
    "                self.vectorstore = FAISS.load_local(\n",
    "                    vs_path, embeddings,\n",
    "                    allow_dangerous_deserialization=True\n",
    "                )\n",
    "                print(f\"      Vector Store: loaded ({self.vectorstore.index.ntotal} vectors)\")\n",
    "            else:\n",
    "                # Create empty store with a placeholder\n",
    "                self.vectorstore = FAISS.from_documents(\n",
    "                    [Document(page_content=\"initialization\", metadata={\"type\": \"system\"})],\n",
    "                    embeddings\n",
    "                )\n",
    "                print(\"      Vector Store: created new (empty)\")\n",
    "        except Exception as e:\n",
    "            print(f\"       Vector Store init failed: {e}\")\n",
    "            self.vectorstore = None\n",
    "\n",
    "    def search_similar_articles(self, query: str, k: int = 5) -> List[dict]:\n",
    "        \"\"\"Search long-term memory for similar past articles.\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            return []\n",
    "        try:\n",
    "            docs = self.vectorstore.similarity_search(query, k=k)\n",
    "            return [\n",
    "                {\"content\": d.page_content, \"metadata\": d.metadata}\n",
    "                for d in docs if d.metadata.get(\"type\") != \"system\"\n",
    "            ]\n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "    def store_articles(self, articles: List[dict]):\n",
    "        \"\"\"Save new articles to long-term vector memory.\"\"\"\n",
    "        if not self.vectorstore:\n",
    "            return\n",
    "        docs = []\n",
    "        for article in articles:\n",
    "            content = f\"{article.get('headline', '')}: {article.get('summary', '')}\"\n",
    "            # Dedup check via hash\n",
    "            content_hash = hashlib.md5(content.encode()).hexdigest()\n",
    "            docs.append(Document(\n",
    "                page_content=content,\n",
    "                metadata={\n",
    "                    \"type\": \"article\",\n",
    "                    \"url\": article.get(\"url\", \"\"),\n",
    "                    \"category\": article.get(\"category\", \"\"),\n",
    "                    \"source\": article.get(\"source\", \"\"),\n",
    "                    \"hash\": content_hash,\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                }\n",
    "            ))\n",
    "        if docs:\n",
    "            self.vectorstore.add_documents(docs)\n",
    "            self.vectorstore.save_local(self.config[\"vectorstore_path\"])\n",
    "            print(f\"     üíæ Stored {len(docs)} articles in long-term memory\")\n",
    "\n",
    "    # ‚îÄ‚îÄ 3. SEMANTIC MEMORY (Knowledge Base) ‚îÄ‚îÄ\n",
    "\n",
    "    def get_known_entities(self) -> Dict[str, dict]:\n",
    "        \"\"\"Retrieve known entities from semantic memory.\"\"\"\n",
    "        return self.knowledge_base.get(\"entities\", {})\n",
    "\n",
    "    def update_entities(self, new_entities: Dict[str, dict]):\n",
    "        \"\"\"Add/update entities in semantic memory.\"\"\"\n",
    "        self.knowledge_base[\"entities\"].update(new_entities)\n",
    "        self._save_json(self.config[\"knowledge_base_path\"], self.knowledge_base)\n",
    "\n",
    "    def get_source_reliability(self, source: str) -> float:\n",
    "        \"\"\"Get reliability score for a source (0.0 to 1.0).\"\"\"\n",
    "        return self.knowledge_base.get(\"source_reliability\", {}).get(source, 0.5)\n",
    "\n",
    "    def update_source_reliability(self, source: str, score: float):\n",
    "        \"\"\"Update source reliability score.\"\"\"\n",
    "        if \"source_reliability\" not in self.knowledge_base:\n",
    "            self.knowledge_base[\"source_reliability\"] = {}\n",
    "        self.knowledge_base[\"source_reliability\"][source] = round(score, 2)\n",
    "        self._save_json(self.config[\"knowledge_base_path\"], self.knowledge_base)\n",
    "\n",
    "    # ‚îÄ‚îÄ 4. EPISODIC MEMORY (Run History) ‚îÄ‚îÄ\n",
    "\n",
    "    def get_recent_episodes(self, n: int = 5) -> List[dict]:\n",
    "        \"\"\"Retrieve the N most recent run episodes.\"\"\"\n",
    "        return self.episodes[-n:]\n",
    "\n",
    "    def get_past_lessons(self) -> List[str]:\n",
    "        \"\"\"Extract actionable lessons from past episodes.\"\"\"\n",
    "        lessons = []\n",
    "        # Defensive check: ensure self.episodes is a list\n",
    "        if not isinstance(self.episodes, list):\n",
    "            print(f\"    WARNING: Episodes memory ({self.config['episodes_path']}) is not a list ({type(self.episodes)}), re-initializing to empty list.\")\n",
    "            self.episodes = [] # Force it to be a list\n",
    "            # Optionally, save the corrected empty list to disk if you suspect file corruption\n",
    "            # self._save_json(self.config[\"episodes_path\"], self.episodes)\n",
    "\n",
    "        for ep in self.episodes[-10:]:\n",
    "            if ep.get(\"qa_feedback\") and not ep.get(\"qa_passed_first_try\", True):\n",
    "                lessons.append(f\"Run {ep.get('run_id', '?')}: {ep['qa_feedback']}\")\n",
    "        return lessons[-5:]  # Keep last 5 lessons\n",
    "\n",
    "    def log_episode(self, episode: dict):\n",
    "        \"\"\"Record a new episode to episodic memory.\"\"\"\n",
    "        episode[\"timestamp\"] = datetime.now().isoformat()\n",
    "        episode[\"run_id\"] = f\"run_{len(self.episodes) + 1:03d}\"\n",
    "        self.episodes.append(episode)\n",
    "        self._save_json(self.config[\"episodes_path\"], self.episodes)\n",
    "        print(f\"     Logged episode: {episode['run_id']}\")\n",
    "\n",
    "    # ‚îÄ‚îÄ 5. PROCEDURAL MEMORY (Strategies) ‚îÄ‚îÄ\n",
    "\n",
    "    def get_best_query_patterns(self) -> List[str]:\n",
    "        \"\"\"Retrieve successful search query patterns.\"\"\"\n",
    "        return self.strategies.get(\"best_query_patterns\", [])\n",
    "\n",
    "    def get_preferred_template(self) -> str:\n",
    "        \"\"\"Get the preferred HTML template style.\"\"\"\n",
    "        return self.strategies.get(\"preferred_html_template\", \"\")\n",
    "\n",
    "    def get_writer_tips(self) -> List[str]:\n",
    "        \"\"\"Get accumulated writer improvement tips.\"\"\"\n",
    "        return self.strategies.get(\"writer_tips\", [])\n",
    "\n",
    "    def update_strategies(self, qa_score: int, qa_feedback: str, queries_used: List[str]):\n",
    "        \"\"\"Update procedural memory based on QA results.\"\"\"\n",
    "        s = self.strategies\n",
    "        s[\"total_runs\"] = s.get(\"total_runs\", 0) + 1\n",
    "        total = s[\"total_runs\"]\n",
    "\n",
    "        # Running average of quality scores\n",
    "        prev_avg = s.get(\"avg_quality_score\", 0)\n",
    "        s[\"avg_quality_score\"] = round(((prev_avg * (total - 1)) + qa_score) / total, 2)\n",
    "\n",
    "        # Store successful queries (score >= 7)\n",
    "        if qa_score >= 7 and queries_used:\n",
    "            existing = set(s.get(\"best_query_patterns\", []))\n",
    "            for q in queries_used[:3]:\n",
    "                existing.add(q)\n",
    "            s[\"best_query_patterns\"] = list(existing)[-20:]  # Keep top 20\n",
    "\n",
    "        # Store writer tips from QA feedback\n",
    "        if qa_feedback and qa_score < 7:\n",
    "            tips = s.get(\"writer_tips\", [])\n",
    "            tips.append(qa_feedback[:200])\n",
    "            s[\"writer_tips\"] = tips[-10:]  # Keep last 10\n",
    "\n",
    "        self._save_json(self.config[\"strategies_path\"], s)\n",
    "        print(f\"       Strategies updated (avg score: {s['avg_quality_score']})\")\n",
    "\n",
    "\n",
    "# Initialize the Memory Manager\n",
    "memory = MemoryManager(CONFIG)\n",
    "print(\"\\n Memory system fully initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1771080989900,
     "user": {
      "displayName": "Partha Bhowmick",
      "userId": "16205602225789129380"
     },
     "user_tz": 0
    },
    "id": "RPDBEnzaCGl6",
    "outputId": "1def3569-d93b-438b-cefc-a4e8ef2a14dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentState schema defined (with memory fields)\n"
     ]
    }
   ],
   "source": [
    "# Step 5: DEFINE SHARED STATE SCHEMA (with Memory Fields)\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"Shared state with memory-enhanced fields.\"\"\"\n",
    "\n",
    "    # ‚îÄ‚îÄ Input ‚îÄ‚îÄ\n",
    "    topic: str\n",
    "\n",
    "    # ‚îÄ‚îÄ Short-Term Memory (working state per run) ‚îÄ‚îÄ\n",
    "    search_queries: List[str]\n",
    "    raw_search_results: List[dict]\n",
    "    articles: List[dict]\n",
    "    page_title: str\n",
    "    page_description: str\n",
    "    html_content: str\n",
    "    qa_passed: bool\n",
    "    qa_feedback: str\n",
    "    qa_score: int\n",
    "    revision_count: int\n",
    "    agent_log: Annotated[List[str], operator.add]\n",
    "\n",
    "    # ‚îÄ‚îÄ Memory-Enhanced Fields ‚îÄ‚îÄ\n",
    "    similar_past_articles: List[dict]    # from long-term memory (FAISS)\n",
    "    known_entities: Dict[str, dict]      # from semantic memory (KB)\n",
    "    past_run_lessons: List[str]          # from episodic memory\n",
    "    preferred_template: str              # from procedural memory\n",
    "    best_query_patterns: List[str]       # from procedural memory\n",
    "    writer_tips: List[str]               # from procedural memory\n",
    "\n",
    "print(\"AgentState schema defined (with memory fields)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1771081002829,
     "user": {
      "displayName": "Partha Bhowmick",
      "userId": "16205602225789129380"
     },
     "user_tz": 0
    },
    "id": "fo773FDkCZT_"
   },
   "outputs": [],
   "source": [
    "# Step 6: MEMORY NODES ‚Äî Load & Save Memory at Key Transitions\n",
    "\n",
    "def load_memory_node(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "    Memory Load Node\n",
    "    Runs BEFORE agents start. Retrieves relevant context from all memory types\n",
    "    and injects it into the shared state.\n",
    "    \"\"\"\n",
    "    print(\" [Memory] Loading context from all memory types...\")\n",
    "\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # Long-term: find similar past articles\n",
    "    similar = memory.search_similar_articles(topic, k=5)\n",
    "    print(f\"    Long-term: found {len(similar)} similar past articles\")\n",
    "\n",
    "    # Semantic: load known entities\n",
    "    entities = memory.get_known_entities()\n",
    "    print(f\"    Semantic: {len(entities)} known entities\")\n",
    "\n",
    "    # Episodic: extract lessons from past runs\n",
    "    lessons = memory.get_past_lessons()\n",
    "    print(f\"    Episodic: {len(lessons)} lessons from past runs\")\n",
    "\n",
    "    # Procedural: load strategies\n",
    "    best_patterns = memory.get_best_query_patterns()\n",
    "    template = memory.get_preferred_template()\n",
    "    tips = memory.get_writer_tips()\n",
    "    print(f\"     Procedural: {len(best_patterns)} query patterns, {len(tips)} writer tips\")\n",
    "\n",
    "    return {\n",
    "        \"similar_past_articles\": similar,\n",
    "        \"known_entities\": entities,\n",
    "        \"past_run_lessons\": lessons,\n",
    "        \"best_query_patterns\": best_patterns,\n",
    "        \"preferred_template\": template,\n",
    "        \"writer_tips\": tips,\n",
    "        \"agent_log\": [f\"[Memory Load] Retrieved context from all 5 memory types\"],\n",
    "    }\n",
    "\n",
    "\n",
    "def save_memory_node(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "     Memory Save Node\n",
    "    Runs AFTER the QA agent. Persists new knowledge back to long-term,\n",
    "    semantic, episodic, and procedural memory.\n",
    "    \"\"\"\n",
    "    print(\" [Memory] Persisting new knowledge...\")\n",
    "\n",
    "    # Long-term: store new articles\n",
    "    articles = state.get(\"articles\", [])\n",
    "    if articles:\n",
    "        memory.store_articles(articles)\n",
    "\n",
    "    # Semantic: extract and store new entities\n",
    "    try:\n",
    "        entity_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"Extract key entities from these article headlines.\n",
    "Return JSON: {{\"entities\": {{\"EntityName\": {{\"type\": \"company|person|product|org|concept\", \"context\": \"brief note\"}}}}}}\n",
    "Only include clearly identifiable entities. Return 5-15 max.\"\"\"),\n",
    "            (\"human\", \"{headlines}\")\n",
    "        ])\n",
    "        headlines = \"\\n\".join([a.get(\"headline\", \"\") for a in articles[:10]])\n",
    "        chain = entity_prompt | llm | JsonOutputParser()\n",
    "        result = chain.invoke({\"headlines\": headlines})\n",
    "        new_entities = result.get(\"entities\", {})\n",
    "        if new_entities:\n",
    "            memory.update_entities(new_entities)\n",
    "            print(f\"    Semantic: added {len(new_entities)} entities\")\n",
    "    except Exception as e:\n",
    "        print(f\"     Entity extraction failed: {e}\")\n",
    "\n",
    "    # Episodic: log this run\n",
    "    memory.log_episode({\n",
    "        \"topic\": state.get(\"topic\", \"\"),\n",
    "        \"num_articles\": len(articles),\n",
    "        \"qa_passed_first_try\": state.get(\"revision_count\", 0) <= 1,\n",
    "        \"qa_score\": state.get(\"qa_score\", 0),\n",
    "        \"qa_feedback\": state.get(\"qa_feedback\", \"\"),\n",
    "        \"revision_count\": state.get(\"revision_count\", 0),\n",
    "        \"queries_used\": state.get(\"search_queries\", []),\n",
    "    })\n",
    "\n",
    "    # Procedural: update strategies\n",
    "    memory.update_strategies(\n",
    "        qa_score=state.get(\"qa_score\", 7),\n",
    "        qa_feedback=state.get(\"qa_feedback\", \"\"),\n",
    "        queries_used=state.get(\"search_queries\", []),\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"agent_log\": [f\"[Memory Save] Persisted to LTM, semantic, episodic, procedural\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1771081042763,
     "user": {
      "displayName": "Partha Bhowmick",
      "userId": "16205602225789129380"
     },
     "user_tz": 0
    },
    "id": "R2g-Aqe5CnbJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 7: AGENT 1 ‚Äî ORCHESTRATOR (Memory-Enhanced)\n",
    "\n",
    "def orchestrator_agent(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "     Orchestrator Agent (Memory-Enhanced)\n",
    "    Uses episodic memory (past lessons) and procedural memory (best patterns)\n",
    "    to generate smarter search queries.\n",
    "    \"\"\"\n",
    "    print(\" [Orchestrator] Planning with memory context...\")\n",
    "\n",
    "    # Pull memory context\n",
    "    past_lessons = state.get(\"past_run_lessons\", [])\n",
    "    best_patterns = state.get(\"best_query_patterns\", [])\n",
    "    similar_articles = state.get(\"similar_past_articles\", [])\n",
    "\n",
    "    lessons_text = \"\\n\".join(past_lessons) if past_lessons else \"No past lessons yet.\"\n",
    "    patterns_text = \"\\n\".join(best_patterns[:5]) if best_patterns else \"No proven patterns yet.\"\n",
    "    seen_topics = \"\\n\".join([a.get(\"content\", \"\")[:100] for a in similar_articles[:3]]) if similar_articles else \"No prior coverage.\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a strategic research planner for an AI news system.\n",
    "Generate 3-5 specific, diverse search queries for the given topic.\n",
    "\n",
    "MEMORY CONTEXT ‚Äî use this to make smarter queries:\n",
    "- Past QA lessons (avoid these mistakes): {lessons}\n",
    "- Previously successful query patterns: {patterns}\n",
    "- Topics already covered (avoid duplicates): {seen_topics}\n",
    "\n",
    "Return ONLY valid JSON: {{\"queries\": [\"query 1\", \"query 2\", \"query 3\"]}}\"\"\"),\n",
    "        (\"human\", \"Topic: {topic}\\nDate: {date}\")\n",
    "    ])\n",
    "\n",
    "    chain = prompt | llm | JsonOutputParser()\n",
    "    result = chain.invoke({\n",
    "        \"topic\": state[\"topic\"],\n",
    "        \"date\": datetime.now().strftime(\"%B %d, %Y\"),\n",
    "        \"lessons\": lessons_text,\n",
    "        \"patterns\": patterns_text,\n",
    "        \"seen_topics\": seen_topics,\n",
    "    })\n",
    "\n",
    "    queries = result.get(\"queries\", [state[\"topic\"]])\n",
    "    print(f\"   üìã Generated {len(queries)} queries (memory-informed)\")\n",
    "    for i, q in enumerate(queries, 1):\n",
    "        print(f\"      {i}. {q}\")\n",
    "\n",
    "    return {\n",
    "        \"search_queries\": queries,\n",
    "        \"revision_count\": state.get(\"revision_count\", 0),\n",
    "        \"agent_log\": [f\"[Orchestrator] {len(queries)} memory-informed queries\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1771081049569,
     "user": {
      "displayName": "Partha Bhowmick",
      "userId": "16205602225789129380"
     },
     "user_tz": 0
    },
    "id": "TiRQC2nBCz5G"
   },
   "outputs": [],
   "source": [
    "# Step 8: AGENT 2 ‚Äî RESEARCH AGENT (Memory-Enhanced)\n",
    "\n",
    "def research_agent(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "     Research Agent (Memory-Enhanced)\n",
    "    Checks long-term memory for duplicates before adding results.\n",
    "    \"\"\"\n",
    "    print(\" [Research Agent] Searching (with dedup from long-term memory)...\")\n",
    "\n",
    "    all_results = []\n",
    "    seen_urls = set()\n",
    "\n",
    "    # Get URLs of articles already in long-term memory\n",
    "    similar_past = state.get(\"similar_past_articles\", [])\n",
    "    known_urls = {a.get(\"metadata\", {}).get(\"url\", \"\") for a in similar_past}\n",
    "\n",
    "    for query in state[\"search_queries\"]:\n",
    "        try:\n",
    "            results = search_tool.invoke(query)\n",
    "            for r in results:\n",
    "                url = r.get(\"url\", \"\")\n",
    "                if url not in seen_urls and url not in known_urls:\n",
    "                    seen_urls.add(url)\n",
    "                    all_results.append({\n",
    "                        \"title\": r.get(\"title\", \"Untitled\"),\n",
    "                        \"url\": url,\n",
    "                        \"content\": r.get(\"content\", \"\"),\n",
    "                        \"query\": query,\n",
    "                    })\n",
    "                elif url in known_urls:\n",
    "                    print(f\"     Skipped (already in memory): {url[:60]}\")\n",
    "            print(f\"   ‚úì '{query}' ‚Üí {len(results)} results\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚úó '{query}' ‚Üí Error: {e}\")\n",
    "\n",
    "    print(f\"    Total new unique results: {len(all_results)}\")\n",
    "\n",
    "    return {\n",
    "        \"raw_search_results\": all_results,\n",
    "        \"agent_log\": [f\"[Research] {len(all_results)} new results (deduped against memory)\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1771081054557,
     "user": {
      "displayName": "Partha Bhowmick",
      "userId": "16205602225789129380"
     },
     "user_tz": 0
    },
    "id": "L5QpVtxQC-7i"
   },
   "outputs": [],
   "source": [
    "# Step 9: AGENT 3 ‚Äî WRITER AGENT (Memory-Enhanced)\n",
    "\n",
    "def writer_agent(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "     Writer Agent (Memory-Enhanced)\n",
    "    Uses semantic memory for entity consistency and episodic memory for lessons.\n",
    "    \"\"\"\n",
    "    print(\"  [Writer Agent] Synthesizing (with memory context)...\")\n",
    "\n",
    "    # Memory context\n",
    "    entities = state.get(\"known_entities\", {})\n",
    "    lessons = state.get(\"past_run_lessons\", [])\n",
    "    writer_tips = state.get(\"writer_tips\", [])\n",
    "\n",
    "    # Build memory hints\n",
    "    entity_hint = \"\"\n",
    "    if entities:\n",
    "        sample = dict(list(entities.items())[:10])\n",
    "        entity_hint = f\"\\nKNOWN ENTITIES (use consistent naming): {json.dumps(sample, default=str)}\"\n",
    "\n",
    "    tips_hint = \"\"\n",
    "    if writer_tips:\n",
    "        tips_hint = f\"\\nWRITER TIPS FROM PAST QA (follow these): \" + \" | \".join(writer_tips[-3:])\n",
    "\n",
    "    revision_note = \"\"\n",
    "    if state.get(\"qa_feedback\") and state.get(\"revision_count\", 0) > 0:\n",
    "        revision_note = f\"\\n\\nQA FEEDBACK TO ADDRESS: {state['qa_feedback']}\"\n",
    "\n",
    "    search_data = json.dumps(state[\"raw_search_results\"][:15], indent=2, default=str)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are an expert AI news journalist.\n",
    "Transform raw search results into polished article summaries.\n",
    "{entity_hint}{tips_hint}{revision_note}\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\n",
    "  \"page_title\": \"Compelling title\",\n",
    "  \"page_description\": \"Brief description\",\n",
    "  \"articles\": [\n",
    "    {{\n",
    "      \"headline\": \"Headline\",\n",
    "      \"summary\": \"2-3 sentence summary\",\n",
    "      \"category\": \"Research|Industry|Policy|Product|Open Source\",\n",
    "      \"source\": \"Source name\",\n",
    "      \"url\": \"URL\",\n",
    "      \"importance\": \"high|medium|low\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Include 5-10 articles, sorted by importance.\"\"\"),\n",
    "        (\"human\", \"Search data:\\n{search_data}\")\n",
    "    ])\n",
    "\n",
    "    chain = prompt | llm | JsonOutputParser()\n",
    "    result = chain.invoke({\n",
    "        \"search_data\": search_data,\n",
    "        \"entity_hint\": entity_hint,\n",
    "        \"tips_hint\": tips_hint,\n",
    "        \"revision_note\": revision_note,\n",
    "    })\n",
    "\n",
    "    articles = result.get(\"articles\", [])\n",
    "    print(f\"    Generated {len(articles)} articles (entity-consistent, lesson-aware)\")\n",
    "\n",
    "    return {\n",
    "        \"articles\": articles,\n",
    "        \"page_title\": result.get(\"page_title\", \"AI News Roundup\"),\n",
    "        \"page_description\": result.get(\"page_description\", \"\"),\n",
    "        \"agent_log\": [f\"[Writer] {len(articles)} articles (memory-enhanced)\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1771081059967,
     "user": {
      "displayName": "Partha Bhowmick",
      "userId": "16205602225789129380"
     },
     "user_tz": 0
    },
    "id": "bGcoSfh3DLzV"
   },
   "outputs": [],
   "source": [
    "# Step 10: AGENT 4 ‚Äî HTML BUILDER (Memory-Enhanced)\n",
    "\n",
    "def html_builder_agent(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "     HTML Builder Agent (Memory-Enhanced)\n",
    "    Loads preferred template from procedural memory if available.\n",
    "    \"\"\"\n",
    "    print(\"  [HTML Builder] Generating page...\")\n",
    "\n",
    "    articles = state.get(\"articles\", [])\n",
    "    page_title = state.get(\"page_title\", \"AI News\")\n",
    "    page_desc = state.get(\"page_description\", \"\")\n",
    "    preferred_template = state.get(\"preferred_template\", \"\")\n",
    "\n",
    "    template_hint = \"\"\n",
    "    if preferred_template:\n",
    "        template_hint = f\"Use this preferred style approach: {preferred_template[:300]}\"\n",
    "        print(\"   üìê Using preferred template from procedural memory\")\n",
    "\n",
    "    # Build article HTML\n",
    "    colors = {\"Research\":\"#7b61ff\",\"Industry\":\"#00d4aa\",\"Policy\":\"#ff6b9d\",\"Product\":\"#ffaa40\",\"Open Source\":\"#4ecdc4\"}\n",
    "\n",
    "    cards_html = \"\"\n",
    "    for i, art in enumerate(articles):\n",
    "        cat = art.get(\"category\", \"General\")\n",
    "        col = colors.get(cat, \"#888\")\n",
    "        imp = art.get(\"importance\", \"medium\")\n",
    "        badge_css = \"font-size:12px;padding:4px 12px;\" if imp == \"high\" else \"font-size:11px;padding:3px 10px;\"\n",
    "\n",
    "        cards_html += f\"\"\"\n",
    "        <article class=\"card\" style=\"animation-delay:{i*0.08}s\">\n",
    "          <div class=\"badge\" style=\"background:{col}15;color:{col};border:1px solid {col}33;{badge_css}\">{cat}</div>\n",
    "          <h2 class=\"card-title\">{art.get(\"headline\",\"Untitled\")}</h2>\n",
    "          <p class=\"card-summary\">{art.get(\"summary\",\"\")}</p>\n",
    "          <div class=\"card-footer\">\n",
    "            <span class=\"card-source\">{art.get(\"source\",\"\")}</span>\n",
    "            <a href=\"{art.get(\"url\",\"#\")}\" target=\"_blank\" class=\"card-link\">Read Full Article ‚Üí</a>\n",
    "          </div>\n",
    "        </article>\"\"\"\n",
    "\n",
    "    gen_date = datetime.now().strftime(\"%B %d, %Y at %I:%M %p\")\n",
    "    run_num = memory.strategies.get(\"total_runs\", 0) + 1\n",
    "    num_entities = len(memory.knowledge_base.get(\"entities\", {}))\n",
    "\n",
    "    html = f\"\"\"<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "<meta charset=\"UTF-8\">\n",
    "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "<title>{page_title}</title>\n",
    "<link href=\"https://fonts.googleapis.com/css2?family=Outfit:wght@300;400;500;600;700&family=Newsreader:wght@400;600;700&display=swap\" rel=\"stylesheet\">\n",
    "<style>\n",
    "  :root {{ --bg:#0b0b12; --sf:#13131c; --bd:#252538; --tx:#e4e4f0; --dm:#8e8eac; --ac:#7b61ff; }}\n",
    "  *{{margin:0;padding:0;box-sizing:border-box;}}\n",
    "  body{{font-family:'Outfit',sans-serif;background:var(--bg);color:var(--tx);line-height:1.65;}}\n",
    "  .hero{{text-align:center;padding:72px 24px 56px;background:linear-gradient(180deg,#161626,var(--bg));border-bottom:1px solid var(--bd);}}\n",
    "  .hero-badge{{display:inline-flex;align-items:center;gap:8px;padding:5px 14px;border:1px solid var(--ac);border-radius:100px;font-size:11px;letter-spacing:2px;text-transform:uppercase;color:var(--ac);margin-bottom:22px;}}\n",
    "  .hero-badge::before{{content:'';width:6px;height:6px;border-radius:50%;background:var(--ac);animation:blink 2s infinite;}}\n",
    "  .hero h1{{font-family:'Newsreader',serif;font-size:clamp(30px,5vw,52px);font-weight:700;line-height:1.12;margin-bottom:14px;background:linear-gradient(135deg,#fff,#bbb);-webkit-background-clip:text;-webkit-text-fill-color:transparent;}}\n",
    "  .hero p{{font-size:15px;color:var(--dm);max-width:580px;margin:0 auto 8px;}}\n",
    "  .hero .meta{{font-size:11px;color:#555;}}\n",
    "  .memory-bar{{display:flex;justify-content:center;gap:16px;margin-top:16px;flex-wrap:wrap;}}\n",
    "  .memory-stat{{display:flex;align-items:center;gap:6px;padding:4px 12px;background:rgba(123,97,255,.06);border:1px solid rgba(123,97,255,.15);border-radius:8px;font-size:11px;color:var(--ac);}}\n",
    "  .wrap{{max-width:880px;margin:0 auto;padding:36px 24px 72px;}}\n",
    "  .grid{{display:grid;gap:18px;}}\n",
    "  .card{{background:var(--sf);border:1px solid var(--bd);border-radius:14px;padding:26px;transition:transform .3s,box-shadow .3s;animation:fadeUp .5s ease both;}}\n",
    "  .card:hover{{transform:translateY(-3px);box-shadow:0 12px 40px rgba(0,0,0,.3);}}\n",
    "  .badge{{display:inline-block;border-radius:6px;font-weight:600;margin-bottom:12px;}}\n",
    "  .card-title{{font-size:19px;font-weight:700;line-height:1.3;margin-bottom:8px;}}\n",
    "  .card-summary{{font-size:13px;color:var(--dm);line-height:1.7;margin-bottom:14px;}}\n",
    "  .card-footer{{display:flex;justify-content:space-between;align-items:center;padding-top:12px;border-top:1px solid var(--bd);}}\n",
    "  .card-source{{font-size:11px;color:#555;font-weight:500;}}\n",
    "  .card-link{{font-size:12px;color:var(--ac);text-decoration:none;font-weight:500;}}\n",
    "  .card-link:hover{{text-decoration:underline;}}\n",
    "  footer{{text-align:center;padding:28px;border-top:1px solid var(--bd);font-size:11px;color:#444;}}\n",
    "  @keyframes fadeUp{{from{{opacity:0;transform:translateY(14px)}}to{{opacity:1;transform:translateY(0)}}}}\n",
    "  @keyframes blink{{0%,100%{{opacity:1}}50%{{opacity:.3}}}}\n",
    "  @media(max-width:600px){{.hero{{padding:44px 16px 32px;}}.card{{padding:20px;}}.card-footer{{flex-direction:column;gap:6px;align-items:flex-start;}}}}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "  <div class=\"hero\">\n",
    "    <div class=\"hero-badge\">AI-Curated News Digest</div>\n",
    "    <h1>{page_title}</h1>\n",
    "    <p>{page_desc}</p>\n",
    "    <p class=\"meta\">Generated {gen_date} ¬∑ Multi-Agent AI System ¬∑ Run #{run_num}</p>\n",
    "    <div class=\"memory-bar\">\n",
    "      <div class=\"memory-stat\">üß¨ {num_entities} entities in knowledge base</div>\n",
    "      <div class=\"memory-stat\">üìñ {len(memory.episodes)} past runs learned from</div>\n",
    "      <div class=\"memory-stat\">‚öôÔ∏è Avg quality: {memory.strategies.get('avg_quality_score', 'N/A')}/10</div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"wrap\">\n",
    "    <div class=\"grid\">{cards_html}</div>\n",
    "  </div>\n",
    "  <footer>Built by Multi-Agent AI System with Memory ¬∑ LangChain + LangGraph</footer>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "\n",
    "    print(f\"    HTML generated ({len(html):,} chars)\")\n",
    "\n",
    "    return {\n",
    "        \"html_content\": html,\n",
    "        \"agent_log\": [f\"[HTML Builder] Page with {len(articles)} articles\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1771081065156,
     "user": {
      "displayName": "Partha Bhowmick",
      "userId": "16205602225789129380"
     },
     "user_tz": 0
    },
    "id": "Vb5SmiuFDddw"
   },
   "outputs": [],
   "source": [
    "# Step  11: AGENT 5 ‚Äî QA AGENT (Memory-Enhanced)\n",
    "\n",
    "\n",
    "def qa_agent(state: AgentState) -> dict:\n",
    "    \"\"\"\n",
    "     QA Agent (Memory-Enhanced)\n",
    "    Compares against past quality scores. Logs episode. Updates strategies.\n",
    "    \"\"\"\n",
    "    print(\" [QA Agent] Reviewing (with historical context)...\")\n",
    "\n",
    "    revision_count = state.get(\"revision_count\", 0)\n",
    "    articles = state.get(\"articles\", [])\n",
    "\n",
    "    # Get past performance for context\n",
    "    recent_eps = memory.get_recent_episodes(3)\n",
    "    past_scores = [ep.get(\"qa_score\", 0) for ep in recent_eps if ep.get(\"qa_score\")]\n",
    "    avg_past = sum(past_scores) / len(past_scores) if past_scores else 0\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a strict QA reviewer for an AI news page.\n",
    "\n",
    "Historical context: Past runs averaged {avg_past}/10 quality.\n",
    "Current revision: #{revision_count} of max {max_revisions}.\n",
    "\n",
    "Review for: content quality, completeness (min 5 articles), HTML validity, design.\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\"passed\": true/false, \"score\": 1-10, \"feedback\": \"specific actionable feedback\"}}\n",
    "\n",
    "Be reasonable ‚Äî minor issues pass. Only fail for significant problems.\"\"\"),\n",
    "        (\"human\", \"Articles: {num_articles}\\nHTML length: {html_len}\\nSample: {sample}\")\n",
    "    ])\n",
    "\n",
    "    sample = json.dumps(articles[0], indent=2) if articles else \"{}\"\n",
    "    chain = prompt | llm | JsonOutputParser()\n",
    "    result = chain.invoke({\n",
    "        \"avg_past\": round(avg_past, 1),\n",
    "        \"revision_count\": revision_count,\n",
    "        \"max_revisions\": CONFIG[\"max_revisions\"],\n",
    "        \"num_articles\": len(articles),\n",
    "        \"html_len\": len(state.get(\"html_content\", \"\")),\n",
    "        \"sample\": sample,\n",
    "    })\n",
    "\n",
    "    passed = result.get(\"passed\", True)\n",
    "    score = result.get(\"score\", 7)\n",
    "    feedback = result.get(\"feedback\", \"\")\n",
    "\n",
    "    if revision_count >= CONFIG[\"max_revisions\"]:\n",
    "        passed = True\n",
    "        feedback = \"Max revisions reached ‚Äî approving.\"\n",
    "\n",
    "    status = \"APPROVED ‚úÖ\" if passed else \"NEEDS REVISION ‚ö†Ô∏è\"\n",
    "    print(f\"   üìä Score: {score}/10 ‚Äî {status} (past avg: {avg_past:.1f})\")\n",
    "    if feedback:\n",
    "        print(f\"   üí¨ {feedback[:120]}\")\n",
    "\n",
    "    return {\n",
    "        \"qa_passed\": passed,\n",
    "        \"qa_score\": score,\n",
    "        \"qa_feedback\": feedback,\n",
    "        \"revision_count\": revision_count + 1,\n",
    "        \"agent_log\": [f\"[QA] Score: {score}/10 ‚Äî {status}\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1771081069303,
     "user": {
      "displayName": "Partha Bhowmick",
      "userId": "16205602225789129380"
     },
     "user_tz": 0
    },
    "id": "H-yXZoB_DpPE",
    "outputId": "bed7ba2d-5947-4993-b274-3ec1a4fa1fa2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LangGraph workflow compiled with memory nodes!\n",
      "   Flow: load_memory ‚Üí orchestrator ‚Üí research ‚Üí writer ‚Üí html_builder ‚Üí qa_review\n",
      "   Memory: load_memory (start) + save_memory (end)\n",
      "   Loop: qa_review ‚Üí writer (if revision needed)\n"
     ]
    }
   ],
   "source": [
    "# Step 12: BUILD THE LANGGRAPH WORKFLOW (with Memory Nodes)\n",
    "\n",
    "\n",
    "def should_revise(state: AgentState) -> str:\n",
    "    \"\"\"Conditional edge: revise or save & finish.\"\"\"\n",
    "    if state.get(\"qa_passed\", False):\n",
    "        return \"save\"\n",
    "    else:\n",
    "        return \"revise\"\n",
    "\n",
    "\n",
    "# Build the graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Agent nodes\n",
    "workflow.add_node(\"load_memory\", load_memory_node)\n",
    "workflow.add_node(\"orchestrator\", orchestrator_agent)\n",
    "workflow.add_node(\"research\", research_agent)\n",
    "workflow.add_node(\"writer\", writer_agent)\n",
    "workflow.add_node(\"html_builder\", html_builder_agent)\n",
    "workflow.add_node(\"qa_review\", qa_agent)\n",
    "workflow.add_node(\"save_memory\", save_memory_node)\n",
    "\n",
    "# Edges with memory nodes interleaved\n",
    "workflow.add_edge(START, \"load_memory\")           # ‚ë† Load all memory\n",
    "workflow.add_edge(\"load_memory\", \"orchestrator\")  # ‚ë° Plan with memory\n",
    "workflow.add_edge(\"orchestrator\", \"research\")     # ‚ë¢ Search the web\n",
    "workflow.add_edge(\"research\", \"writer\")           # ‚ë£ Write content\n",
    "workflow.add_edge(\"writer\", \"html_builder\")       # ‚ë§ Build HTML\n",
    "workflow.add_edge(\"html_builder\", \"qa_review\")    # ‚ë• QA review\n",
    "\n",
    "# Conditional: QA ‚Üí save & end, or ‚Üí revise\n",
    "workflow.add_conditional_edges(\n",
    "    \"qa_review\",\n",
    "    should_revise,\n",
    "    {\n",
    "        \"save\": \"save_memory\",   # ‚úÖ Passed ‚Üí persist memory ‚Üí END\n",
    "        \"revise\": \"writer\",      # ‚ö†Ô∏è Failed ‚Üí back to writer\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"save_memory\", END)             # ‚ë¶ Done!\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "print(\" LangGraph workflow compiled with memory nodes!\")\n",
    "print(\"   Flow: load_memory ‚Üí orchestrator ‚Üí research ‚Üí writer ‚Üí html_builder ‚Üí qa_review\")\n",
    "print(\"   Memory: load_memory (start) + save_memory (end)\")\n",
    "print(\"   Loop: qa_review ‚Üí writer (if revision needed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCVJJUnYD5N-"
   },
   "outputs": [],
   "source": [
    "# Step 13:  RUN THE PIPELINE\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\" LAUNCHING MULTI-AGENT PIPELINE (with Memory)\")\n",
    "print(f\" Topic: {CONFIG['topic']}\")\n",
    "print(f\" Memory: {len(memory.episodes)} past episodes | {len(memory.knowledge_base.get('entities', {}))} entities\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "final_state = graph.invoke({\n",
    "    \"topic\": CONFIG[\"topic\"],\n",
    "    \"search_queries\": [],\n",
    "    \"raw_search_results\": [],\n",
    "    \"articles\": [],\n",
    "    \"page_title\": \"\",\n",
    "    \"page_description\": \"\",\n",
    "    \"html_content\": \"\",\n",
    "    \"qa_passed\": False,\n",
    "    \"qa_feedback\": \"\",\n",
    "    \"qa_score\": 0,\n",
    "    \"revision_count\": 0,\n",
    "    \"agent_log\": [\"[System] Pipeline started with memory\"],\n",
    "    \"similar_past_articles\": [],\n",
    "    \"known_entities\": {},\n",
    "    \"past_run_lessons\": [],\n",
    "    \"preferred_template\": \"\",\n",
    "    \"best_query_patterns\": [],\n",
    "    \"writer_tips\": [],\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\" PIPELINE COMPLETE\")\n",
    "print(f\"   Revisions: {final_state['revision_count']}\")\n",
    "print(f\"   Articles:  {len(final_state['articles'])}\")\n",
    "print(f\"   QA Score:  {final_state.get('qa_score', 'N/A')}/10\")\n",
    "print(f\"   HTML size: {len(final_state['html_content']):,} chars\")\n",
    "print(f\"   Entities:  {len(memory.knowledge_base.get('entities', {}))} in KB\")\n",
    "print(f\"   Episodes:  {len(memory.episodes)} total\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "print(\"\\n Agent Log:\")\n",
    "for entry in final_state.get(\"agent_log\", []):\n",
    "    print(f\"   {entry}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cVa7JDMZEBm-"
   },
   "outputs": [],
   "source": [
    "# Step 14: DISPLAY THE OUTPUT\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "display(HTML(final_state[\"html_content\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1771081154420,
     "user": {
      "displayName": "Partha Bhowmick",
      "userId": "16205602225789129380"
     },
     "user_tz": 0
    },
    "id": "hObhA95qEMcb",
    "outputId": "619df832-103c-438a-9a46-ddbaf79bffcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " HTML saved: ai_news_page.html\n",
      " Memory persisted to: ./agent_memory/\n",
      "   ‚îú‚îÄ‚îÄ vectorstore/    (FAISS long-term memory)\n",
      "   ‚îú‚îÄ‚îÄ knowledge_base.json (semantic memory)\n",
      "   ‚îú‚îÄ‚îÄ episodes.json   (episodic memory)\n",
      "   ‚îî‚îÄ‚îÄ strategies.json (procedural memory)\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_c1a4f9d9-0c6d-4ec4-b8a2-4dedab6701b0\", \"ai_news_page.html\", 11941)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Download triggered!\n"
     ]
    }
   ],
   "source": [
    "# Step 15: SAVE & DOWNLOAD\n",
    "\n",
    "output_path = \"ai_news_page.html\"\n",
    "with open(output_path, \"w\") as f:\n",
    "    f.write(final_state[\"html_content\"])\n",
    "\n",
    "print(f\" HTML saved: {output_path}\")\n",
    "print(f\" Memory persisted to: {CONFIG['memory_dir']}/\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ vectorstore/    (FAISS long-term memory)\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ knowledge_base.json (semantic memory)\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ episodes.json   (episodic memory)\")\n",
    "print(f\"   ‚îî‚îÄ‚îÄ strategies.json (procedural memory)\")\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download(output_path)\n",
    "    print(\" Download triggered!\")\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1771072583567,
     "user": {
      "displayName": "Partha Bhowmick",
      "userId": "16205602225789129380"
     },
     "user_tz": 0
    },
    "id": "0aBmgJiPAiGE",
    "outputId": "88ca2284-394f-45ff-ecec-d7aa59b23eba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " MEMORY INSPECTION\n",
      "==================================================\n",
      "\n",
      " SEMANTIC MEMORY ‚Äî Known Entities:\n",
      "   ‚Ä¢ AI: {'type': 'concept', 'context': 'Artificial Intelligence, a major focus in technology trends and breakthroughs.'}\n",
      "   ‚Ä¢ Data Science: {'type': 'concept', 'context': 'A field related to AI, involving data analysis and interpretation.'}\n",
      "   ‚Ä¢ Technology Industry: {'type': 'concept', 'context': 'The sector involving the development and manufacturing of technology products or services.'}\n",
      "   ‚Ä¢ Tech: {'type': 'concept', 'context': 'Short for technology, often used to describe the industry and its trends.'}\n",
      "\n",
      " EPISODIC MEMORY ‚Äî Last 3 Runs:\n",
      "   ‚Ä¢ run_001 | Score: 8/10 | Articles: 9 | First-try pass: False\n",
      "\n",
      "  PROCEDURAL MEMORY ‚Äî Strategies:\n",
      "   Avg Quality Score: 8.0/10\n",
      "   Total Runs: 1\n",
      "   Best Query Patterns: ['AI breakthroughs in 2025 and 2026', 'Impact of AI on industries in 2025 and 2026', 'Latest AI technology advancements February 2026']\n",
      "   Writer Tips: []\n",
      "\n",
      " LONG-TERM MEMORY (Vector Store):\n",
      "   Vectors stored: 10\n"
     ]
    }
   ],
   "source": [
    "# Step 16: (BONUS) INSPECT MEMORY STATE\n",
    "\n",
    "print(\" MEMORY INSPECTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n SEMANTIC MEMORY ‚Äî Known Entities:\")\n",
    "for name, info in list(memory.knowledge_base.get(\"entities\", {}).items())[:10]:\n",
    "    print(f\"   ‚Ä¢ {name}: {info}\")\n",
    "\n",
    "print(f\"\\n EPISODIC MEMORY ‚Äî Last 3 Runs:\")\n",
    "for ep in memory.episodes[-3:]:\n",
    "    print(f\"   ‚Ä¢ {ep.get('run_id', '?')} | Score: {ep.get('qa_score', '?')}/10 | \"\n",
    "          f\"Articles: {ep.get('num_articles', '?')} | \"\n",
    "          f\"First-try pass: {ep.get('qa_passed_first_try', '?')}\")\n",
    "\n",
    "print(f\"\\n  PROCEDURAL MEMORY ‚Äî Strategies:\")\n",
    "print(f\"   Avg Quality Score: {memory.strategies.get('avg_quality_score', 'N/A')}/10\")\n",
    "print(f\"   Total Runs: {memory.strategies.get('total_runs', 0)}\")\n",
    "print(f\"   Best Query Patterns: {memory.strategies.get('best_query_patterns', [])[:5]}\")\n",
    "print(f\"   Writer Tips: {memory.strategies.get('writer_tips', [])[:3]}\")\n",
    "\n",
    "print(f\"\\n LONG-TERM MEMORY (Vector Store):\")\n",
    "if memory.vectorstore:\n",
    "    print(f\"   Vectors stored: {memory.vectorstore.index.ntotal}\")\n",
    "else:\n",
    "    print(\" Not initialized\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOHbAy41FbVgqARqOgpI82j",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
